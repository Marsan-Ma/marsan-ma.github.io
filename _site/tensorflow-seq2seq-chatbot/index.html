<!DOCTYPE html> <!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--> <!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--> <!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--> <!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>A deep learning seq2seq model ChatBot in tensorflow &#8211; Marsan Ma's info</title> <meta name="description" content="Marsan Ma's info"> <meta name="keywords" content="chatbot, deep learning, neural network, rnn"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:image" content="https://4.bp.blogspot.com/-aArS0l1pjHQ/Vjj71pKAaEI/AAAAAAAAAxE/Nvy1FSbD_Vs/s640/2TFstaticgraphic_alt-01.png"> <meta name="twitter:title" content="A deep learning seq2seq model ChatBot in tensorflow"> <meta name="twitter:description" content="A deep-learning chatbot with (seq2seq model + attention mechanism + beam_search algorithm + anti-language model) in tensorflow, works end-to-end from training corpus to chat model, and build-in a facebook-messenger backend server."> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="A deep learning seq2seq model ChatBot in tensorflow"> <meta property="og:description" content="A deep-learning chatbot with (seq2seq model + attention mechanism + beam_search algorithm + anti-language model) in tensorflow, works end-to-end from training corpus to chat model, and build-in a facebook-messenger backend server."> <meta property="og:url" content="http://localhost:4000/tensorflow-seq2seq-chatbot/"> <meta property="og:site_name" content="Marsan Ma's info"> <meta property="og:image" content="http://localhost:4000/assets/img/logo.png"> <link rel="canonical" href="http://localhost:4000/tensorflow-seq2seq-chatbot/"> <link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Marsan Ma's info Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css"> <!-- JS --> <script src="http://localhost:4000/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="http://localhost:4000/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="http://localhost:4000/favicon.png" /> <link rel="shortcut icon" href="http://localhost:4000/favicon.ico" /> <!-- Background Image --> <style type="text/css">body {background-image:url(http://localhost:4000/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> <style type="text/css">.feature {background-image:url(https://4.bp.blogspot.com/-aArS0l1pjHQ/Vjj71pKAaEI/AAAAAAAAAxE/Nvy1FSbD_Vs/s640/2TFstaticgraphic_alt-01.png);}</style> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="http://localhost:4000/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="http://localhost:4000/assets/img/logo.png" alt="Marsan Ma's info photo" class="author-photo"> <h4>Marsan Ma's info</h4> <p>Marsan Ma's info</p> </li> <li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="http://linkedin.com/in/marsanma" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a> </li> <li> <a href="http://github.com/Marsan-Ma" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> <li> <a href="mailto:marsan@gmail.com" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> </ul><!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="http://localhost:4000/posts/">All Posts</a></li> <li><a href="http://localhost:4000/tags/">All Tags</a></li> </ul> </li> <li><a href="http://localhost:4000/projects/" >Projects</a></li> </ul><!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title feature "> <h1>A deep learning seq2seq model ChatBot in tensorflow</h1> <h4>02 Oct 2016</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~6 minutes </p><!-- /.entry-reading-time --> <a class="btn zoombtn" href="http://localhost:4000/projects/"> <i class="fa fa-chevron-left"></i> </a> </div> <h1 id="tensorflow-chatbot">Tensorflow chatbot</h1> <h3 id="with-seq2seq--attention--dict-compress--beam-search--anti-lm--facebook-messenger-server">(with seq2seq + attention + dict-compress + beam search + anti-LM + facebook messenger server)</h3> <p>Github Repository: <a href="https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm">https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm</a></p> <blockquote> <p>####[Update 2017-03-14]</p> <ol> <li>Upgrade to tensorflow v1.0.0, no backward compatible since tensorflow have changed so much.</li> <li>A pre-trained model with twitter corpus is added, just <code class="highlighter-rouge">./go_example</code> to chat! (or preview my <a href="https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm/blob/master/example_chat.md">chat example</a>)</li> <li>You could start from tracing this <code class="highlighter-rouge">go_example</code> script to know how things work!</li> </ol> </blockquote> <h2 id="briefing">Briefing</h2> <p>This is a <a href="http://arxiv.org/abs/1406.1078">seq2seq model</a> modified from <a href="https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html">tensorflow example</a>.</p> <ol> <li>The original tensorflow seq2seq has <a href="http://arxiv.org/abs/1412.7449">attention mechanism</a> implemented out-of-box.</li> <li>And speedup training by <a href="https://arxiv.org/pdf/1412.2007v2.pdf">dictionary space compressing</a>, then decompressed by projection the embedding while decoding.</li> <li>This work add option to do <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> in decoding procedure, which usually find better, more interesting response.</li> <li>Added <a href="https://arxiv.org/abs/1510.03055">anti-language model</a> to suppress the generic response problem of intrinsic seq2seq model.</li> <li>Imeplemented <a href="https://arxiv.org/abs/1606.01541">this deep reinforcement learning architecture</a> as an option to enhence semantic coherence and perplexity of response.</li> <li>A light weight <a href="http://flask.pocoo.org/">Flask</a> server <code class="highlighter-rouge">app.py</code> is included to be the Facebook Messenger App backend.</li> </ol> <h2 id="in-laymans-terms">In Layman’s terms</h2> <p>I explained some detail about the features and some implementation tricks <a href="https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm/blob/master/README2.md">here</a>.</p> <h2 id="just-tell-me-how-it-works">Just tell me how it works</h2> <h4 id="clone-the-repository">Clone the repository</h4> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm.git
</code></pre></div></div> <h4 id="prepare-for-corpus">Prepare for Corpus</h4> <p>You may find corpus such as twitter chat, open movie subtitle, or ptt forums from <a href="https://github.com/Marsan-Ma/chat_corpus">my chat corpus repository</a>. You need to put it under path like:</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf_chatbot_seq2seq_antilm/works/&lt;YOUR_MODEL_NAME&gt;/data/train/chat.txt
</code></pre></div></div> <p>And hand craft some testing sentences (each sentence per line) in:</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf_chatbot_seq2seq_antilm/works/&lt;YOUR_MODEL_NAME&gt;/data/test/test_set.txt
</code></pre></div></div> <h4 id="train-the-model">Train the model</h4> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 main.py --mode train --model_name &lt;MODEL_NAME&gt;
</code></pre></div></div> <h4 id="run-some-test-example-and-see-the-bot-response">Run some test example and see the bot response</h4> <p>after you trained your model until perplexity under 50 or so, you could do:</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 main.py --mode test --model_name &lt;MODEL_NAME&gt;
</code></pre></div></div> <p><strong>[Note!!!] if you put any parameter overwrite in this main.py commmand, be sure to apply both to train and test, or just modify in lib/config.py for failsafe.</strong></p> <h2 id="start-your-facebook-messenger-backend-server">Start your Facebook Messenger backend server</h2> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 app.py --model_name &lt;MODEL_NAME&gt;
</code></pre></div></div> <p>You may see this <a href="https://github.com/Marsan-Ma/fb_messenger">minimum fb_messenger example</a> for more details like setting up SSL, webhook, and work-arounds for known bug.</p> <p>Here’s an interesting comparison: The left conversation enabled beam search with beam = 10, the response is barely better than always “i don’t know”. The right conversation also used beam search and additionally, enabled anti-language model. This supposed to suppress generic response, and the response do seems better.</p> <p><img src="https://raw.githubusercontent.com/Marsan-Ma/tf_chatbot_seq2seq_antilm/master/doc/messenger.png" alt="messenger.png" /></p> <h2 id="deep-reinforcement-learning">Deep reinforcement learning</h2> <blockquote> <p>[Update 2017-03-09] Reinforcement learning does not work now, wait for fix.</p> </blockquote> <p>If you want some chance to further improve your model, here I implemented a reinforcement learning architecture inspired by <a href="https://arxiv.org/abs/1606.01541">Li et al., 2016</a>. Just enable the reinforce_learn option in <code class="highlighter-rouge">config.py</code>, you might want to add your own rule in <code class="highlighter-rouge">step_rf()</code> function in <code class="highlighter-rouge">lib/seq2seq_mode.py</code>.</p> <p>Note that you should <strong>train in normal mode to get a decent model first!</strong>, since the reinforcement learning will explore the brave new world with this pre-trained model. It will end up taking forever to improve itself if you start with a bad model.</p> <h2 id="introduction">Introduction</h2> <p>Seq2seq is a great model released by <a href="http://arxiv.org/abs/1406.1078">Cho et al., 2014</a>. At first it’s used to do machine translation, and soon people find that anything about <strong>mapping something to another thing</strong> could be also achieved by seq2seq model. Chatbot is one of these miracles, where we consider consecutive dialog as some kind of “mapping” relationship.</p> <p>Here is the classic intro picture show the seq2seq model architecture, quote from this <a href="http://googleresearch.blogspot.ru/2015/11/computer-respond-to-this-email.html">blogpost about gmail auto-reply feature</a>.</p> <p><a href="http://4.bp.blogspot.com/-aArS0l1pjHQ/Vjj71pKAaEI/AAAAAAAAAxE/Nvy1FSbD_Vs/s640/2TFstaticgraphic_alt-01.png"><img src="http://4.bp.blogspot.com/-aArS0l1pjHQ/Vjj71pKAaEI/AAAAAAAAAxE/Nvy1FSbD_Vs/s640/2TFstaticgraphic_alt-01.png" alt="seq2seq" /></a></p> <p>The problem is, so far we haven’t find a better objective function for chatbot. We are still using <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE (maximum likelyhood estimation)</a>, which is doing good for machine translation, but always generate generic response like “me too”, “I think so”, “I love you” while doing chat.</p> <p>These responses are not informative, but they do have large probability — since they tend to appear many times in training corpus. We don’t won’t our chatbot always replying these noncense, so we need to find some way to make our bot more “interesting”, technically speaking, to increase the “perplexity” of reponse.</p> <p>Here we reproduce the work of <a href="http://arxiv.org/pdf/1510.03055v3.pdf">Li. et al., 2016</a> try to solve this problem. The main idea is using the same seq2seq model as a language model, to get the candidate words with high probability in each decoding timestamp as a anti-model, then we penalize these words always being high probability for any input. By this anti-model, we could get more special, non-generic, informative response.</p> <p>The original work of <a href="http://arxiv.org/pdf/1510.03055v3.pdf">Li. et al</a> use <a href="http://delivery.acm.org/10.1145/1080000/1075117/p160-och.pdf">MERT (Och, 2003)</a> with <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> as metrics to find the best probability weighting (the <strong>λ</strong> and <strong>γ</strong> in <strong>Score(T) = p(T|S) − λU(T) + γNt</strong>) of the corresponding anti-language model. But I find that BLEU score in chat corpus tend to always being zero, thus can’t get meaningful result here. If anyone has any idea about this, drop me a message, thanks!</p> <h2 id="parameters">Parameters</h2> <p>There are some options to for model training and predicting in lib/config.py. Basically they are self-explained and could work with default value for most of cases. Here we only list something you need to config:</p> <p><strong>About environment</strong></p> <table> <thead> <tr> <th>name</th> <th>type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>mode</td> <td>string</td> <td>work mode: train/test/chat</td> </tr> <tr> <td>model_name</td> <td>string</td> <td>model name, affects your working path (storing the data, nn_model, result folders)</td> </tr> <tr> <td>scope_name</td> <td>string</td> <td>In tensorflow if you need to load two graph at the same time, you need to save/load them in different namespace. (If you need only one seq2seq model, leave it as default)</td> </tr> <tr> <td>vocab_size</td> <td>integer</td> <td>depends on your corpus language: for english, 60000 is good enough. For chinese you need at least 100000 or 200000.</td> </tr> <tr> <td>gpu_usage</td> <td>float</td> <td>tensorflow gpu memory fraction used, default is 1 and tensorflow will occupy 100% of your GPU. If you have multi jobs sharing your GPU resource, make it 0.5 or 0.3, for 2 or 3 jobs.</td> </tr> <tr> <td>reinforce_learn</td> <td>int</td> <td>set 1 to enable reinforcement learning mode</td> </tr> </tbody> </table> <p><strong>About decoding</strong></p> <table> <thead> <tr> <th>name</th> <th>type</th> <th>default</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>beam_size</td> <td>int</td> <td>10</td> <td>beam search size, setting 1 equals to greedy search</td> </tr> <tr> <td>antilm</td> <td>float</td> <td>0 (disabled)</td> <td>punish weight of <a href="http://arxiv.org/pdf/1510.03055v3.pdf">anti-language model</a></td> </tr> <tr> <td>n_bonus</td> <td>float</td> <td>0 (disabled)</td> <td>reward weight of sentence length</td> </tr> </tbody> </table> <p>The anti-LM functin is disabled by default, you may start from setting antilm=0.5~0.7 and n_bonus=0.05 to see if you like the difference in results.</p> <h2 id="requirements">Requirements</h2> <ol> <li> <p>For training, GPU is recommended since seq2seq is a large model, you need certain computing power to do the training and predicting efficiently, especially when you set a large beam-search size.</p> </li> <li> <p>DRAM requirement is not strict as CPU/GPU, since we are doing stochastic gradient decent.</p> </li> <li> <p>If you are new to deep-learning, setting-up things like GPU, python environment is annoying to you, here are dockers of my machine learning environment:<br /> <a href="https://github.com/Marsan-Ma/docker_mldm">(non-gpu version docker)</a> / <a href="https://github.com/Marsan-Ma/docker_mldm_gpu">(gpu version docker)</a></p> </li> </ol> <h2 id="references">References</h2> <p>Seq2seq is a model with many preliminaries, I’ve been spend quite some time surveying and here are some best materials which benefit me a lot:</p> <ol> <li> <p>The best blogpost explaining RNN, LSTM, GRU and seq2seq model: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> by Christopher Olah.</p> </li> <li> <p>This work <a href="https://github.com/sherjilozair/char-rnn-tensorflow">sherjilozair/char-rnn-tensorflow</a> helps me learn a lot about language model and implementation graph in tensorflow.</p> </li> <li> <p>If you are interested in more magic about RNN, here is a MUST-READ blogpost: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> by Andrej Karpathy.</p> </li> <li> <p>The vanilla version seq2seq+attention: <a href="https://github.com/nicolas-ivanov/tf_seq2seq_chatbot">nicolas-ivanov/tf_seq2seq_chatbot</a>. This will help you figure out the main flow of vanilla seq2seq model, and I build this repository based on this work.</p> </li> </ol> <h2 id="todos">TODOs</h2> <ol> <li> <p>Currently I build beam-search out of graph, which means — it’s very slow. There are discussions about build it in-graph <a href="https://github.com/tensorflow/tensorflow/issues/654#issuecomment-196168030">here</a> and <a href="https://github.com/tensorflow/tensorflow/pull/3756">there</a>. But unfortunately if you want add something more than beam-search, like this anti-LM work, you need much more than just beam search to be in-graph.</p> </li> <li> <p>I haven’t figure out how the MERT with BLEU can optimize weight of anti-LM model, since currently the BLEU is often being zero.</p> </li> </ol> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="http://localhost:4000/tags/#chatbot" title="Pages tagged chatbot" class="tag"><span class="term">chatbot</span></a><a href="http://localhost:4000/tags/#deep learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a><a href="http://localhost:4000/tags/#neural network" title="Pages tagged neural network" class="tag"><span class="term">neural network</span></a><a href="http://localhost:4000/tags/#rnn" title="Pages tagged rnn" class="tag"><span class="term">rnn</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/tensorflow-seq2seq-chatbot/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Like</span> </a> <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/tensorflow-seq2seq-chatbot/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=http://localhost:4000/tensorflow-seq2seq-chatbot/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> </header> <!-- JS --> <script src="http://localhost:4000/assets/js/jquery-1.12.0.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.dlmenu.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.goup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.magnific-popup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.fitvid.min.js"></script> <script src="http://localhost:4000/assets/js/scripts.js"></script> <script src="http://localhost:4000/assets/js/site.js"></script> <!-- Asynchronous Google Analytics snippet --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-32764178-10', 'auto'); ga('require', 'linkid', 'linkid.js'); ga('send', 'pageview'); </script> <script type="text/javascript"> var disqus_shortname = 'marsan-ma'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
